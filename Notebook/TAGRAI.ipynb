{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d508eb52",
   "metadata": {},
   "source": [
    "# TAGRAI: Thematic Analysis of Genre using AI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f824eb6f",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Humans don’t usually need to be told the genre of a movie - we **infer** it naturally through **context**, **patterns** and **past experiences**. Even a short plot summary gives us enough clues to make educated guesses.\n",
    "\n",
    "We subconsciously pick up on **keywords**, **themes**, **emotions** and **narrative tropes**. For example:\n",
    "\n",
    "- *\"After losing his family in a tragic accident, a man retreats to the wilderness and learns to survive.\"*  \n",
    "  - Likely **Adventure**, maybe **Drama**\n",
    "\n",
    "- *\"A young girl stumbles upon a cursed mirror that shows her a dark version of herself.\"*  \n",
    "  - Sounds like **Horror**, **Mystery**, **Thriller**\n",
    "\n",
    "- *\"Two mismatched cops must team up to stop a bank heist on Christmas Eve.\"*  \n",
    "  - Probably **Action**, maybe **Comedy** or **Buddy Cop**\n",
    "\n",
    "---\n",
    "\n",
    "### What Clues Do Humans Use?\n",
    "\n",
    "Humans rely on a mix of subtle and familiar cues:\n",
    "\n",
    "- **Lexical cues**: Words like _\"curse\"_, _\"revenge\"_, _\"love\"_, _\"haunted\"_\n",
    "- **Emotional tone**:  \n",
    "  - _Tragic_ --> **Drama**  \n",
    "  - _Light-hearted_ --> **Comedy**\n",
    "- **Narrative structure**:  \n",
    "  - _hero’s journey_ --> **Adventure**  \n",
    "  - _mysterious_ --> **Mystery**\n",
    "- **Cultural tropes**:  \n",
    "  - _Mismatched partners_ --> **Buddy Cop**  \n",
    "  - _Apocalypse or AI takeover_ --> **Sci-Fi**\n",
    "\n",
    "---\n",
    "\n",
    "> **So can a machine guess a movie’s genre just by reading its plot?**\n",
    "\n",
    "This project explores that very question.\n",
    "\n",
    "---\n",
    "\n",
    "### Project Goal\n",
    "\n",
    "The aim is to build an **end-to-end machine learning pipeline** that can predict one or more genres of a movie based solely on its **plot synopsis**.\n",
    "\n",
    "Most movies follow recognizable **patterns**:\n",
    "- a love triangle may suggest **Romance**\n",
    "- a car chase hints at **Action**\n",
    "- a haunted house screams **Horror**.  \n",
    "\n",
    "This project attempts to detect and learn those patterns using **Natural Language Processing (NLP)** and **Multi-label Classification**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9be5e24",
   "metadata": {},
   "source": [
    "## Data Collection\n",
    "\n",
    "The data for this project has been collected from [IMDB's public dump](https://www.kaggle.com/datasets/ashirwadsangwan/imdb-dataset/), [The IMDB Bollywood movies dataset](https://www.kaggle.com/datasets/anoopjohny/the-imdb-bollywood-movies-dataset) and [Metadata on ~5,000 movies from TMDb](https://www.kaggle.com/datasets/tmdb/tmdb-movie-metadata). IMDB contains data on approximately ~5 million movies. But just for this project we are going to work with ~50k movies for learning purposes.\n",
    "\n",
    "---\n",
    "\n",
    "### Purpose\n",
    "\n",
    "Our first source of data comes from IMDB's public data dump file ([title.basics.tsv](https://www.kaggle.com/datasets/ashirwadsangwan/imdb-dataset/?select=title.basics.tsv)). For extracting useful data from this I wrote a script [`genreplot_harvester.py`](../Scripts/genreplot_harvester.py) where my goal is to:\n",
    "1. Read IMDb movie titles from `title.basics.tsv` (official IMDb dataset)\n",
    "2. Fetch each movie’s plot and genres using IMDbPY\n",
    "3. Store results in `raw_data.csv` while supporting resume after interruption\n",
    "4. Run in **parallel (multi-threaded)** to speed up scraping\n",
    "5. Save intermediate progress using pickle files and logs\n",
    "\n",
    "---\n",
    "<!-- \n",
    "### Step-by-Step Walkthrough\n",
    "\n",
    "#### 1. Imports\n",
    "```python\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from imdb import IMDb\n",
    "import time\n",
    "import csv\n",
    "import random\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from threading import Lock\n",
    "from tqdm import tqdm\n",
    "```\n",
    "\n",
    "- `os`: File path handling.\n",
    "- `pickle`: Store Python objects (e.g., titles, movie data) for resume.\n",
    "- `pandas`: Load IMDb TSV and CSV files.\n",
    "- `IMDb (IMDbPY)`: Library for fetching IMDb movie details.\n",
    "- `ThreadPoolExecutor`: Enables concurrent requests.\n",
    "- `Lock`: Prevents multiple threads from writing to the same file/data at the same time.\n",
    "- `tqdm`: Progress bar for user feedback.\n",
    "\n",
    "#### 2. Config\n",
    "\n",
    "```python\n",
    "TOP_N = 5320016  # Number of movies to scrape = 5320016 (This is the maximum amount)\n",
    "THREADS = 4\n",
    "RETRY_LIMIT = 3\n",
    "DELAY_RANGE = (0.8, 1.4)\n",
    "\n",
    "TITLE_CACHE = \"DATA/cache/titles.pkl\"\n",
    "MOVIE_CACHE = \"DATA/cache/movie_cache.pkl\"\n",
    "PROGRESS_LOG = \"DATA/cache/progress.log\"\n",
    "RAW_CSV = \"DATA/raw/raw_data.csv\"\n",
    "TSV_PATH = \"DATA/raw/title.basics.tsv\"\n",
    "```\n",
    "\n",
    "Here we initialize the filepaths and variables to be used later on.  \n",
    "> **NOTE**: Caching paths allow resuming scraping without starting over.\n",
    "\n",
    "#### 3. Directory checks\n",
    "\n",
    "```python\n",
    "# Ensure directories exist\n",
    "os.makedirs(\"DATA/cache\", exist_ok=True)\n",
    "os.makedirs(\"DATA/raw\", exist_ok=True)\n",
    "\n",
    "# Check for TSV existence\n",
    "if not os.path.exists(TSV_PATH):\n",
    "    raise FileNotFoundError(f\"Missing required TSV file: {TSV_PATH}\")\n",
    "```\n",
    "\n",
    "Ensures directories exist and stops execution if the IMDb dataset is missing.\n",
    "\n",
    "#### 4. Loading Titles\n",
    "\n",
    "```python\n",
    "if os.path.exists(TITLE_CACHE):\n",
    "    print(\"Loading cached titles...\")\n",
    "    with open(TITLE_CACHE, \"rb\") as f:\n",
    "        top_titles = pickle.load(f)\n",
    "else:\n",
    "    print(\"Reading IMDb TSV file...\")\n",
    "    df = pd.read_csv(TSV_PATH, sep='\\t', low_memory=False)\n",
    "    movies = df[df['titleType'] == 'movie'][['tconst', 'primaryTitle']].dropna().reset_index(drop=True)\n",
    "    top_titles = movies.head(TOP_N)\n",
    "    with open(TITLE_CACHE, \"wb\") as f:\n",
    "        pickle.dump(top_titles, f)\n",
    "    print(\"Titles cached.\")\n",
    "```\n",
    "\n",
    "**If cached titles exist**, load from pickle for speed; **else**, read `title.basics.tsv`, filter only `movie` type, take top `TOP_N` entries, and cache them.\n",
    "\n",
    "#### 5. Load Cache & Progess Log\n",
    "\n",
    "```python\n",
    "# IMDbPY Setup\n",
    "ia = IMDb()\n",
    "\n",
    "# Load movie cache\n",
    "if os.path.exists(MOVIE_CACHE):\n",
    "    with open(MOVIE_CACHE, \"rb\") as f:\n",
    "        movie_data = pickle.load(f)\n",
    "else:\n",
    "    movie_data = {}\n",
    "\n",
    "# Resume Progress\n",
    "start_index = 0\n",
    "if os.path.exists(PROGRESS_LOG):\n",
    "    with open(PROGRESS_LOG, \"r\") as f:\n",
    "        try:\n",
    "            start_index = int(f.read().strip())\n",
    "        except ValueError:\n",
    "            start_index = 0\n",
    "```\n",
    "\n",
    "First, create an instance of IMDbPY to fetch movie data, then load cache:\n",
    "- `movie_data`: Dictionary storing scraped movie info (so previously scraped movies aren’t fetched again).\n",
    "- `start_index`: Indicates where to resume scraping.\n",
    "\n",
    "#### 6. CSV Setup\n",
    "\n",
    "```python\n",
    "# CSV Setup\n",
    "csv_lock = Lock()\n",
    "if not os.path.exists(RAW_CSV):\n",
    "    with open(RAW_CSV, \"w\", newline='', encoding=\"utf-8\") as raw_file:\n",
    "        writer = csv.writer(raw_file)\n",
    "        writer.writerow([\"imdb_id\", \"title\", \"plot\", \"genres\"])\n",
    "```\n",
    "\n",
    "Initializes raw_data.csv if it doesn’t exist.\n",
    "\n",
    "#### 7. Thread Locks\n",
    "\n",
    "```python\n",
    "# Locks for thread-safe operations\n",
    "data_lock = Lock()\n",
    "log_lock = Lock()\n",
    "```\n",
    "\n",
    "Used to synchronize writes to `movie_data` (pickle) and `progress.log`.\n",
    "\n",
    "#### 8. Scraping Function\n",
    "\n",
    "```python\n",
    "def scrape_movie(i):\n",
    "    row = top_titles.iloc[i]\n",
    "    imdb_id, title = row['tconst'], row['primaryTitle']\n",
    "\n",
    "    with data_lock:\n",
    "        if imdb_id in movie_data:\n",
    "            return imdb_id, movie_data[imdb_id], i\n",
    "\n",
    "    for attempt in range(RETRY_LIMIT):\n",
    "        try:\n",
    "            movie_id = imdb_id.replace(\"tt\", \"\")\n",
    "            movie = ia.get_movie(movie_id)\n",
    "            plot = movie.get('plot', [None])[0]\n",
    "            genres = ', '.join(movie.get('genres', []))\n",
    "            data = (imdb_id, title, plot, genres)\n",
    "\n",
    "            with data_lock:\n",
    "                movie_data[imdb_id] = data\n",
    "\n",
    "            time.sleep(random.uniform(*DELAY_RANGE))\n",
    "            return imdb_id, data, i\n",
    "        except Exception as e:\n",
    "            if attempt == RETRY_LIMIT - 1:\n",
    "                print(f\"[{i}] Failed: {title} | Error: {e}\")\n",
    "            time.sleep(random.uniform(1.5, 2.0))\n",
    "    return imdb_id, None, i\n",
    "```\n",
    "\n",
    "- Checks if movie already cached, if yes, then skips IMDbPY call.\n",
    "- Retries up to 3 times in case of failure\n",
    "- Randomized sleep prevents getting IP-blocked\n",
    "- Returns (imdb_id, data tuple, index) for writing\n",
    "\n",
    "#### 9. Parallel Scraping\n",
    "\n",
    "```python\n",
    "print(f\"Starting scraping from index {start_index}...\")\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=THREADS) as executor:\n",
    "    futures = {executor.submit(scrape_movie, i): i for i in range(start_index, len(top_titles))}\n",
    "\n",
    "    with open(RAW_CSV, \"a\", newline='', encoding=\"utf-8\") as raw_file:\n",
    "        writer = csv.writer(raw_file)\n",
    "\n",
    "        for count, future in enumerate(tqdm(as_completed(futures), total=len(futures), desc=\"Scraping Movies\"), start=start_index + 1):\n",
    "            try:\n",
    "                imdb_id, data, i = future.result()\n",
    "\n",
    "                if data:\n",
    "                    with csv_lock:\n",
    "                        writer.writerow(data)\n",
    "\n",
    "                # Save every 100 or at the end\n",
    "                if count % 100 == 0 or count == len(futures):\n",
    "                    with data_lock:\n",
    "                        with open(MOVIE_CACHE, \"wb\") as f:\n",
    "                            pickle.dump(movie_data, f)\n",
    "\n",
    "                    with log_lock:\n",
    "                        with open(PROGRESS_LOG, \"w\") as f:\n",
    "                            f.write(str(i + 1))\n",
    "            except Exception as e:\n",
    "                print(f\"Error for index {futures[future]}: {e}\")\n",
    "```\n",
    "\n",
    "- `ThreadPoolExecutor`: Manages concurrent scraping.\n",
    "- `as_completed`: Processes futures as soon as each thread finishes.\n",
    "- `tqdm`: Shows progress bar.\n",
    "- Every 100 movies:\n",
    "    - Saves `movie_data.pkl` (so script can resume).\n",
    "    - Updates `progress.log`.\n",
    "\n",
    "#### 10. Final Save\n",
    "\n",
    "```python\n",
    "with data_lock:\n",
    "    with open(MOVIE_CACHE, \"wb\") as f:\n",
    "        pickle.dump(movie_data, f)\n",
    "\n",
    "with log_lock:\n",
    "    with open(PROGRESS_LOG, \"w\") as f:\n",
    "        f.write(str(len(top_titles)))\n",
    "```\n",
    "\n",
    "Ensures everything is saved even if script ends normally.\n",
    "\n",
    "--- -->\n",
    "\n",
    "### Thought Process\n",
    "\n",
    "1. Goal:\n",
    "    Build a dataset of movies containing:\n",
    "    - Title\n",
    "    - Plot (synopsis)\n",
    "    - Genre(s)*\n",
    "\n",
    "    This dataset will later be used to train a model that predicts genre from plot.\n",
    "\n",
    "2. Challenges:\n",
    "    - IMDb doesn’t provide a free API dump of plots and genres.\n",
    "    - IMDbPY fetches data per movie, which is slow and can get blocked if overloaded.\n",
    "    - Scraping 5M+ movies can take weeks unless optimized.\n",
    "    - Network errors, timeouts, or interruptions could cause loss of progress if not handled carefully.\n",
    "\n",
    "3. Design priorities:\n",
    "    - Resumability: Can stop and start anytime.\n",
    "    - Parallelism: Use multiple threads to speed up scraping while respecting IMDb limits.\n",
    "    - Caching: Avoid refetching data unnecessarily.\n",
    "    - Integrity: Ensure no corruption in CSV or partial writes.\n",
    "    - Filtering: Prepare different CSVs depending on data completeness.\n",
    "\n",
    "<small>*During the development of this script, I had forgotten to add genres to the dataset from title.basics.tsv, I have rectified this later on during the development phase </small>\n",
    "\n",
    "---\n",
    "\n",
    "### Intuition\n",
    "\n",
    "1. IMDb’s official dataset (`title.basics.tsv`) gives a complete list of movies and IDs. It’s reliable and complete, and ensures we have every IMDb ID we need.\n",
    "2. Use IMDbPY for per-movie enrichment to fetch missing data like plot and genres. Why not scrape HTML? IMDbPY handles parsing internally, reducing complexity.\n",
    "3. Agressive caching:\n",
    "    - Title cache: No need to reload TSV every run.\n",
    "    - Movie cache: Avoid duplicate API calls.\n",
    "    - Progress log: Restart where we left off.\n",
    "4. Parallelize carefully:\n",
    "    - IMDb may block rapid requests.\n",
    "    - Solution: Use multiple threads with random delays to keep request frequency low but throughput higher.\n",
    "5. Save in chunks:\n",
    "    - If the script crashes midway, all scraped data is still safe.\n",
    "    - Use pickle and CSV writes every 100 movies.\n",
    "\n",
    "##### **NOTE**: TMDB was considered first\n",
    "\n",
    "TMDB (The Movie Database) offers a well-documented API with free access and it allows querying movies by:\n",
    "- ID or title\n",
    "- Full details including genres, overview (plot), runtime, and popularity.\n",
    "\n",
    "It’s a common go-to for developers working on movie-related ML projects. However it was dropped because at the time of designing this pipeline, TMDB had intermittent API outages and latency spikes. Requests either timed out or returned partial data. Also, TMDB’s free tier imposes strict request limits and TMDB’s movie catalog is large but not as complete as IMDb. IMDb’s `title.basics.tsv` guarantees every registered title, while TMDB’s API may miss obscure or older films.\n",
    "\n",
    "---\n",
    "\n",
    "### Algorithm\n",
    "\n",
    "1. Setup and Validation\n",
    "    - Create required directories (`DATA/cache` and `DATA/raw`).\n",
    "    - Verify that `title.basics.tsv` exists (abort if missing).\n",
    "\n",
    "2. Load Titles\n",
    "    - If cached titles exist (`titles.pkl`), load them.\n",
    "    - Else:\n",
    "        - Read `title.basics.tsv` using pandas.\n",
    "        - Filter rows where `titleType == 'movie'`.\n",
    "        - Keep only `tconst` (IMDb ID) and `primaryTitle`.\n",
    "        - Cache result as pickle for future runs.\n",
    "\n",
    "3. Load Resume Data\n",
    "    - If `movie_cache.pkl` exists, load it.\n",
    "    - If `progress.log` exists, read starting index.\n",
    "    - If `raw_data.csv` does not exist, create it and add headers.\n",
    "\n",
    "4. Define Scraping Function\n",
    "    For each movie (by index `i`):\n",
    "    - Retrieve `imdb_id` and `title`.\n",
    "    - If `imdb_id` exists in `movie_data` cache:\n",
    "        - Return cached result (skip API call).\n",
    "    - Else:\n",
    "        - Try up to `RETRY_LIMIT` times:\n",
    "            - Fetch movie using `ia.get_movie(imdb_id)`.\n",
    "            - Extract `plot` (first entry in list) and `genres` (comma-separated).\n",
    "            - Save to `movie_data` dictionary.\n",
    "            - Sleep for random interval (`DELAY_RANGE`) to avoid being flagged.\n",
    "        - If all retries fail, return `None`.\n",
    "\n",
    "5. Parallel Scraping\n",
    "    - Create a `ThreadPoolExecutor` with `THREADS` workers.\n",
    "    - Submit `scrape_movie(i)` for all movies from `start_index` to `TOP_N`.\n",
    "    - Process futures as they complete:\n",
    "        - Write result to CSV.\n",
    "        - Every 100 movies (or at end):\n",
    "            - Save `movie_data.pkl`.\n",
    "            - Update `progress.log`.\n",
    "\n",
    "6. Final Save\n",
    "Save all remaining movie data and progress index and print completion message.\n",
    "\n",
    "---\n",
    "\n",
    "### Potential Areas of Confusion\n",
    "\n",
    "1. Why use IMDB's `title.basics.tsv`?  \n",
    "**Ans:** It’s the most complete and structured source for all movie IDs and titles and IMDbPY alone can’t fetch every movie without knowing the IDs, and scraping IMDb’s HTML would be inconsistent and risky. Also, TSV allows offline filtering (e.g., only movies, skip TV series, shorts) which massively reduces unnecessary API calls.\n",
    "\n",
    "2. Why cache titles?  \n",
    "**Ans:** Reading the TSV (with millions of rows) every time is slow and memory-heavy. Pickle lets you load filtered movie IDs in seconds instead of parsing the file repeatedly.\n",
    "\n",
    "3. Why cache scraped data (`movie_cache.pkl`) and progress (`progress.log`)?  \n",
    "**Ans:** Scraping thousands of movies is long-running (weeks if done sequentially) and if the script crashes, you’d lose all progress without caching. `progress.log` ensures you resume from last successful index. `movie_cache.pkl` ensures already scraped movies aren’t re-fetched.\n",
    "\n",
    "4. Why use threads but control request frequency?  \n",
    "**Ans:** Sequential scraping for so many movies would take months if not years at 1 second per request, so threads allow **multiple simultaneous requests** but still under IMDb’s radar:  \n",
    "    - `DELAY_RANGE` adds **random wait** to avoid being detected as a bot.\n",
    "    - Using `THREADS = 4` balances speed and safety.\n",
    "\n",
    "5. Why use locks for CSV and cache writes?  \n",
    "**Ans:** Multiple threads writing to the same file simultaneously can cause corruption. Locks ensure one thread writes at a time, avoiding race conditions.\n",
    "\n",
    "6. Why have our save checkpoint at every 100 movies?  \n",
    "**Ans:** If the script crashes, you only lose the last few movies, not the entire dataset but writing after every movie would be too frequent (slows performance).\n",
    "\n",
    "7. Why not scrape everything in one go without all this complexity?  \n",
    "**Ans:** IMDb will block your IP if requests are too fast and long scraping jobs will fail eventually due to network hiccups. Without caching/resuming, any crash means starting over and a clean, resumable, cached approach guarantees data integrity and time savings.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0bbc370",
   "metadata": {},
   "source": [
    "### Merging IMDb Scraped Data with Genres from `title.basics.tsv`\n",
    "\n",
    "The initial scraped dataset (`raw_data.csv`) sometimes contains missing or partial genres. Since `title.basics.tsv` from IMDb already has a `genres` column, combining both ensures maximum genre coverage. Rows without any plot are not useful for genre prediction, so they were removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3ba62649",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading raw_data.csv...\n",
      "Loading title.basics.tsv...\n",
      "Merging genres from title.basics.tsv...\n",
      "Movie_Data.csv created at ../DATA/raw/Movie_Data.csv with 39029 entries\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd #Importing libraries\n",
    "\n",
    "#File Paths\n",
    "RAW_DATA_CSV = \"../DATA/raw/raw_data.csv\"           # Contains imdb_id,title,plot and empty genres\n",
    "TITLE_BASICS_TSV = \"../DATA/raw/title.basics.tsv\"   # IMDb dataset with genres column\n",
    "OUTPUT_CSV = \"../DATA/raw/Movie_Data.csv\"\n",
    "\n",
    "#Load Data\n",
    "print(\"Loading raw_data.csv...\")\n",
    "df_raw = pd.read_csv(RAW_DATA_CSV)\n",
    "\n",
    "print(\"Loading title.basics.tsv...\")\n",
    "df_titles = pd.read_csv(TITLE_BASICS_TSV, sep=\"\\t\", low_memory=False)\n",
    "\n",
    "# Keep only relevant columns\n",
    "df_titles = df_titles[[\"tconst\", \"genres\"]]\n",
    "\n",
    "# Merge genres from title.basics.tsv\n",
    "print(\"Merging genres from title.basics.tsv...\")\n",
    "merged = pd.merge(df_raw, df_titles, how=\"left\", left_on=\"imdb_id\", right_on=\"tconst\")\n",
    "\n",
    "# Replace missing genres from TSV with scraped genres\n",
    "merged[\"final_genres\"] = merged.apply(\n",
    "    lambda row: row[\"genres_y\"] if pd.notna(row[\"genres_y\"]) else row[\"genres_x\"],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Clean genres: remove 'Short' and \"\\N\" if present\n",
    "def clean_genres(genre_str):\n",
    "    if pd.isna(genre_str):\n",
    "        return None\n",
    "    genres = [\n",
    "        g.strip()\n",
    "        for g in genre_str.split(\",\")\n",
    "        if g.strip().lower() not in [\"short\", \"\\\\n\", \"\\\\N\"]\n",
    "    ]\n",
    "    return \", \".join(genres)\n",
    "\n",
    "merged[\"final_genres\"] = merged[\"final_genres\"].apply(clean_genres)\n",
    "\n",
    "# Drop rows with empty or NaN plot\n",
    "merged = merged.dropna(subset=[\"plot\"])\n",
    "merged = merged[merged[\"plot\"].str.strip() != \"\"]\n",
    "\n",
    "# Build final DataFrame\n",
    "movie_data = merged[[\"title\", \"plot\", \"final_genres\"]].rename(\n",
    "    columns={\"final_genres\": \"genres\"}\n",
    ")\n",
    "\n",
    "# Save to CSV\n",
    "movie_data.to_csv(OUTPUT_CSV, index=False, encoding=\"utf-8\")\n",
    "print(f\"Movie_Data.csv created at {OUTPUT_CSV} with {len(movie_data)} entries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6cc7b7",
   "metadata": {},
   "source": [
    "#### Key Steps:\n",
    "\n",
    "1. Load `raw_data.csv` (scraped IMDb data) and `title.basics.tsv` (official IMDb dataset).\n",
    "2. Merge on `imdb_id` to retrieve genres for each movie.\n",
    "3. Prefer genres from `title.basics.tsv` when available; otherwise fall back to scraped genres.\n",
    "4. Clean genres by removing unwanted tags such as `Short` and invalid placeholders like `\\N`. Removing \"Short\" was essential to prevent short films from dominating the training set.\n",
    "5. Drop rows with missing or empty plots.\n",
    "6. Save final merged and cleaned dataset to `Movie_Data.csv`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92368b3",
   "metadata": {},
   "source": [
    "### Appending Additional Datasets while Avoiding Duplicates\n",
    "\n",
    "Now that we have our base dataset, we need to add the remaining data from `movies_en.csv` and `movies_hn.csv` to our `Movie_Data.csv`.\n",
    "\n",
    "I want to enrich the movie dataset with more examples but however plot scraping from IMDB is very time consuming, I decided to use some previous static datasets and throw them into the mix. However, direct appending would introduce duplicates, so duplicate checking by title is required and finally overwriting `Movie_Data.csv` instead of creating new files avoids versioning confusion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "02dcd554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Movie_Data.csv...\n",
      "Loaded 39029 IMDb entries\n",
      "Loading movies_en.csv...\n",
      "Added 4296 English movie entries after duplicate filtering\n",
      "Loading movies_hn.csv...\n",
      "Added 4350 Hindi movie entries after duplicate filtering\n",
      "Appending new movies and rewriting Movie_Data.csv...\n",
      "Movie_Data.csv updated with 47675 total entries\n"
     ]
    }
   ],
   "source": [
    "# File paths\n",
    "MOVIE_DATA_CSV = \"../DATA/raw/Movie_Data.csv\"       # Existing cleaned movie dataset\n",
    "MOVIES_EN_CSV = \"../DATA/raw/movies_en.csv\"         # Extra English movies dataset\n",
    "MOVIES_HN_CSV = \"../DATA/raw/movies_hn.csv\"         # Extra Hindi movies dataset\n",
    "\n",
    "# Load Movie_Data.csv\n",
    "print(\"Loading Movie_Data.csv...\")\n",
    "df_main = pd.read_csv(MOVIE_DATA_CSV)\n",
    "existing_titles = set(df_main[\"title\"].str.lower().str.strip())  # For duplicate lookup\n",
    "print(f\"Loaded {len(df_main)} IMDb entries\")\n",
    "\n",
    "# Load and process movies_en.csv\n",
    "print(\"Loading movies_en.csv...\")\n",
    "df_en = pd.read_csv(MOVIES_EN_CSV)\n",
    "df_en = df_en[[\"title\", \"overview\", \"genres\"]].rename(\n",
    "    columns={\"overview\": \"plot\"}\n",
    ")\n",
    "df_en = df_en.dropna(subset=[\"plot\"])\n",
    "df_en = df_en[df_en[\"plot\"].str.strip() != \"\"]\n",
    "df_en = df_en[~df_en[\"title\"].str.lower().str.strip().isin(existing_titles)]\n",
    "print(f\"Added {len(df_en)} English movie entries after duplicate filtering\")\n",
    "\n",
    "# Load and process movies_hn.csv\n",
    "print(\"Loading movies_hn.csv...\")\n",
    "df_hn = pd.read_csv(MOVIES_HN_CSV)\n",
    "df_hn = df_hn[[\"Film Name\", \"Summary\"]].rename(\n",
    "    columns={\"Film Name\": \"title\", \"Summary\": \"plot\"}\n",
    ")\n",
    "df_hn[\"genres\"] = None  # Hindi dataset doesn't have genres\n",
    "df_hn = df_hn.dropna(subset=[\"plot\"])\n",
    "df_hn = df_hn[df_hn[\"plot\"].str.strip() != \"\"]\n",
    "df_hn = df_hn[~df_hn[\"title\"].str.lower().str.strip().isin(existing_titles)]\n",
    "print(f\"Added {len(df_hn)} Hindi movie entries after duplicate filtering\")\n",
    "\n",
    "# Combine and overwrite Movie_Data.csv\n",
    "print(\"Appending new movies and rewriting Movie_Data.csv...\")\n",
    "final_df = pd.concat([df_main, df_en, df_hn], ignore_index=True)\n",
    "final_df.to_csv(MOVIE_DATA_CSV, index=False, encoding=\"utf-8\")\n",
    "print(f\"Movie_Data.csv updated with {len(final_df)} total entries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7b35fe",
   "metadata": {},
   "source": [
    "#### Key Steps\n",
    "\n",
    "1. Load `Movie_Data.csv` and create a set of all existing titles for fast lookup.\n",
    "2. Load `movies_en.csv` and keep only `title`, `overview`, and `genres`.\n",
    "3. Load `movies_hn.csv` and keep only `Film Name` and `Summary` (no genres available).\n",
    "4. Filter out duplicates (case-insensitive, whitespace-trimmed) by checking if the title exists in `Movie_Data.csv`. Removing duplicates early on saves unnecessary space and ensures balanced dataset distribution.\n",
    "5. Concatenate all dataframes and rewrite `Movie_Data.csv`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deea48d7",
   "metadata": {},
   "source": [
    "### Separating Labeled and Unlabeled Data for Training and Prediction\n",
    "\n",
    "Splitting the datasets early keeps the ML pipeline clean and efficient. Movies with genres form the training dataset for supervised learning. Movies without genres can later be used as test inputs for genre prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "596d1556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Movie_Data.csv...\n",
      "Total entries: 47675\n",
      "Splitting data...\n",
      "Movie_Train.csv created with 42529 entries\n",
      "Movie_Test.csv created with 5146 entries\n"
     ]
    }
   ],
   "source": [
    "# File paths\n",
    "MOVIE_DATA_CSV = \"../DATA/raw/Movie_Data.csv\"\n",
    "MOVIE_TRAIN_CSV = \"../DATA/raw/Movie_Train.csv\"\n",
    "MOVIE_PLOT_ONLY_CSV = \"../DATA/raw/Movie_Test.csv\"\n",
    "\n",
    "# Load Movie_Data.csv\n",
    "print(\"Loading Movie_Data.csv...\")\n",
    "df = pd.read_csv(MOVIE_DATA_CSV)\n",
    "print(f\"Total entries: {len(df)}\")\n",
    "\n",
    "# Split into train set (with genres) and plot-only set\n",
    "print(\"Splitting data...\")\n",
    "df_train = df.dropna(subset=[\"genres\"])\n",
    "df_train = df_train[df_train[\"genres\"].str.strip() != \"\"]\n",
    "df_plot_only = df[df[\"genres\"].isna() | (df[\"genres\"].str.strip() == \"\")]\n",
    "\n",
    "# Keep required columns\n",
    "df_train = df_train[[\"title\", \"plot\", \"genres\"]]\n",
    "df_plot_only = df_plot_only[[\"title\", \"plot\"]]\n",
    "\n",
    "# Save to CSV files\n",
    "df_train.to_csv(MOVIE_TRAIN_CSV, index=False, encoding=\"utf-8\")\n",
    "df_plot_only.to_csv(MOVIE_PLOT_ONLY_CSV, index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(f\"Movie_Train.csv created with {len(df_train)} entries\")\n",
    "print(f\"Movie_Test.csv created with {len(df_plot_only)} entries\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
